// AiChat/core/director.js
import { LLM } from '@/services/llm.js';
import { buildSystemPrompt } from '@/core/prompt-builder.js';
import { 
    ACTOR_SCENE_CONTEXT,
    ACTOR_AUTONOMOUS_MODE // è®°å¾—å¼•å…¥è¿™ä¸ªæ–°å¸¸é‡
} from '@/core/director-prompts.js';



/**
 * ğŸ¤– è‡ªä¸»æ¼”å‘˜ (Autonomous Agent)
 * ä½œç”¨ï¼šè§’è‰²è‡ªå·±å†³å®šæ˜¯å¦å‘è¨€ï¼Œæ— éœ€å¯¼æ¼”è°ƒåº¦ï¼›æ”¯æŒè‡ªä¸»ç§»åŠ¨ã€‚
 * @param {Array} subScenes - å½“å‰åœºæ™¯åˆæ³•çš„å­åŒºåŸŸåˆ—è¡¨
 */
export async function runAutonomousActor({ targetNpc, locationName, formattedTime, userName, activeNpcs, history, allContacts, config, contextLimit, subScenes }) {
    
    // 1. è¯»å–ç§èŠè®°å¿†
    const realProfile = allContacts.find(c => String(c.id) === String(targetNpc.privateChatId));
    const privateMemory = realProfile ? (realProfile.summary || "") : "";

    // 2. æ„å»ºåŸºç¡€ System Prompt
    let charSystemPrompt = buildSystemPrompt({
        role: targetNpc,
        userName: userName,
        summary: privateMemory,
        formattedTime: formattedTime,
        location: locationName, 
        mode: 'face', 
        activity: targetNpc.initialState || 'interactive',
        clothes: targetNpc.clothing || 'default',
        relation: targetNpc.currentRelation || 'acquaintance'
    });

    // 3. è¡¥å……ç¯å¢ƒä¸Šä¸‹æ–‡
    const otherNames = activeNpcs.filter(n => n.id !== targetNpc.id).map(n => n.name).join('ã€');
    const sceneContext = ACTOR_SCENE_CONTEXT
        .replace('{{location_name}}', locationName)
        .replace('{{other_names}}', otherNames || 'æ— ');
    
    charSystemPrompt += `\n${sceneContext}\n`;

    // 4. ğŸ”¥ æ³¨å…¥è‡ªä¸»å†³ç­–æŒ‡ä»¤ (å¸¦åœ°ç‚¹é™åˆ¶) ğŸ”¥
    // å¦‚æœæ²¡æœ‰ä¼  subScenesï¼Œå°±é»˜è®¤ä¸ºå½“å‰åœ°ç‚¹ï¼Œé˜²æ­¢ AI ä¹±è·‘
    const validLocationsStr = (subScenes && subScenes.length > 0) ? subScenes.join('ã€') : locationName;
    
    const autoModeInst = ACTOR_AUTONOMOUS_MODE
        .replace('{{role_name}}', targetNpc.name)
        .replace('{{valid_locations}}', validLocationsStr);

    charSystemPrompt += `\n${autoModeInst}`;

    // 5. æ„å»ºä¸Šä¸‹æ–‡
    const limit = contextLimit || 20;
    const context = history.slice(-limit).map(m => {
         if (m.isSystem) return { role: 'system', content: m.content };
         if (m.role === 'user') return { role: 'user', content: m.content };
         return { role: m.role === targetNpc.name ? 'assistant' : 'user', content: m.content }; 
    });

    console.log(`ğŸ¤– [Auto] ${targetNpc.name} æ­£åœ¨æ€è€ƒ (æ·±åº¦:${limit})...`);

    try {
        const reply = await LLM.chat({
            config,
            messages: context,
            systemPrompt: charSystemPrompt,
            temperature: 0.3 // å…è®¸ä¸€å®šçš„æ€§æ ¼å‘æŒ¥
        });
        
        // 6. å¤„ç†ç»“æœ
        if (reply) {
            const cleanReply = reply.trim();
            // å¦‚æœ AI å†³å®šæ²‰é»˜ï¼Œè¿”å› null
            if (cleanReply.includes('[SILENCE]') || cleanReply === '') {
                console.log(`ğŸ¤– [Auto] ${targetNpc.name} å†³å®šä¿æŒæ²‰é»˜ã€‚`);
                return null;
            }
            // å¦åˆ™è¿”å›å†…å®¹ (å»é™¤å¯èƒ½å­˜åœ¨çš„åå­—å‰ç¼€)
            return cleanReply.replace(new RegExp(`^${targetNpc.name}[:ï¼š]\\s*`, 'i'), '').trim();
        }
    } catch (e) {
        console.error(`[Auto] ${targetNpc.name} æ€è€ƒå¤±è´¥`, e);
    }
    return null;
}